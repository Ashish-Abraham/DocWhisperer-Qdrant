diff --git a/app.py b/app.py
--- a/app.py
+++ b/app.py
@@ -1,58 +1,134 @@
-import datetime
-import os
-import random
-from pathlib import Path
-from typing import Any
-import pandas as pd
-import datasets
-from llama_index.core import (VectorStoreIndex, ServiceContext)
-from llama_index.core.settings import Settings
-from transformers import AutoTokenizer, AutoModel
-from qdrant_client import QdrantClient
-from llama_index.vector_stores.qdrant import QdrantVectorStore
-import streamlit as st
-import ingest  # Import the ingest.py file
-from load_model import load_models  # Import the load_models function
-
-st.set_page_config(page_title="DocWhisperer", page_icon="üßô‚Äç‚ôÇÔ∏è", layout="centered", initial_sidebar_state="auto", menu_items=None)
-client = QdrantClient(host='localhost', port=6333)
-vector_store = QdrantVectorStore(client=client, collection_name="ASM")
-
-@st.cache_resource(show_spinner=False)
-def load_data(_llm):
-    with st.spinner(text="Loading the index ‚Äì hang tight!"):
-        index = VectorStoreIndex.load_from_disk("ASM_index.json", service_context=ServiceContext.from_defaults(_llm=llm))
-    return index
-
-# Add a file uploader for PDF files
-uploaded_files = st.file_uploader("Choose PDF files", type="pdf", accept_multiple_files=True)
-
-if uploaded_files:
-    # Invoke ingest.py with the uploaded PDF files
-    ingest.ingest_pdfs(uploaded_files, client, "ASM")
-    st.success("PDF files successfully ingested!")
-
-if __name__ == '__main__':
-    embed_model, llm = load_models()  # Load the models
-    Settings.llm = llm
-    Settings.embed_model = embed_model
-    st.title("DocWhispererüßô‚Äç‚ôÇÔ∏è")  # Changed the title
-    index = load_data(llm)
-    if "chat_engine" not in st.session_state.keys():
-        # Initialize the chat engine
-        st.session_state.chat_engine = index.as_chat_engine(chat_mode="condense_question", verbose=True)
-    if prompt := st.chat_input("Your question"):
-        # Prompt for user input and save to chat history
-        st.session_state.messages.append({"role": "user", "content": prompt})
-        for message in st.session_state.messages:
-            # Display the prior chat messages
-            with st.chat_message(message["role"]):
-                st.write(message["content"])
-        # If last message is not from assistant, generate a new response
-        if st.session_state.messages[-1]["role"] != "assistant":
-            with st.chat_message("assistant"):
-                with st.spinner("Thinking..."):
-                    response = st.session_state.chat_engine.chat(prompt)
-                    st.write(response.response)
-                    message = {"role": "assistant", "content": response.response}
-                    st.session_state.messages.append(message)
+import datetime
+import os
+import random
+from pathlib import Path
+from typing import Any
+import pandas as pd
+import datasets
+from llama_index.core import (VectorStoreIndex, ServiceContext)
+from llama_index.core.settings import Settings
+from transformers import AutoTokenizer, AutoModel
+from qdrant_client import QdrantClient
+from llama_index.vector_stores.qdrant import QdrantVectorStore
+import streamlit as st
+import ingest  # Import the ingest.py file
+from load_model import load_models  # Import the load_models function
+
+st.set_page_config(page_title="DocWhisperer", page_icon="üßô‚Äç‚ôÇÔ∏è", layout="centered", initial_sidebar_state="auto", menu_items=None)
+# Custom CSS for better UI
+st.set_page_config(
+    page_title="DocWhisperer",
+    page_icon="üßô‚Äç‚ôÇÔ∏è",
+    layout="wide",
+    initial_sidebar_state="auto"
+)
+
+# Apply custom styling
+st.markdown("""
+    <style>
+    .main {
+        padding: 2rem;
+    }
+    .stButton>button {
+        width: 100%;
+        border-radius: 5px;
+        height: 3em;
+        background-color: #9933FF;
+        color: white;
+    }
+    .stTextInput>div>div>input {
+        border-radius: 5px;
+    }
+    .css-1d391kg {
+        padding: 2rem 1rem;
+    }
+    .stAlert {
+        padding: 1rem;
+        border-radius: 5px;
+    }
+    </style>
+""", unsafe_allow_html=True)
+
+# Initialize session state
+if 'messages' not in st.session_state:
+    st.session_state.messages = []
+
+vector_store = QdrantVectorStore(client=client, collection_name="ASM")
+
+@st.cache_resource(show_spinner=False)
+def load_data(_llm):
+    with st.spinner(text="Loading the index ‚Äì hang tight!"):
+    with st.spinner(text="Loading the index ‚Äì hang tight! üîÆ"):
+    return index
+
+# Add a file uploader for PDF files
+# Sidebar
+with st.sidebar:
+    st.image("images/w1_noback.png", width=100)
+    st.title("DocWhisperer")
+    st.markdown("---")
+
+    # File uploader in sidebar
+    uploaded_files = st.file_uploader(
+        "üìÑ Upload your PDF files",
+        type="pdf",
+        accept_multiple_files=True
+    )
+if uploaded_files:
+    if uploaded_files:
+        with st.spinner("Processing your documents... üìö"):
+            try:
+                ingest.ingest_pdfs(uploaded_files, client, "ASM")
+                st.success("‚ú® Documents successfully processed!")
+            except Exception as e:
+                st.error(f"Error processing documents: {str(e)}")
+
+    # Clear chat button
+    if st.button("üßπ Clear Chat"):
+        st.session_state.messages = []
+        st.experimental_rerun()
+if __name__ == '__main__':
+    embed_model, llm = load_models()  # Load the models
+    embed_model, llm = load_models()
+    Settings.embed_model = embed_model
+    st.title("DocWhispererüßô‚Äç‚ôÇÔ∏è")  # Changed the title
+
+    # Main area
+    st.title("DocWhisperer üßô‚Äç‚ôÇÔ∏è")
+
+    # Welcome message
+    if not st.session_state.messages:
+        st.markdown("""
+        ### üëã Welcome to DocWhisperer!
+
+        I'm your magical document assistant. Here's how to get started:
+        1. Upload your PDF documents using the sidebar
+        2. Ask me anything about your documents
+        3. I'll search through them and provide detailed answers
+
+        Let's begin our journey through your documents! ‚ú®
+        """)
+
+    # Initialize chat engine
+    if "chat_engine" not in st.session_state.keys():
+        # Initialize the chat engine
+    if prompt := st.chat_input("Your question"):
+
+    # Chat interface
+    for message in st.session_state.messages:
+        with st.chat_message(message["role"]):
+            st.write(message["content"])
+
+    if prompt := st.chat_input("Ask me anything about your documents... üí≠"):
+        for message in st.session_state.messages:
+        with st.chat_message("user"):
+            st.write(prompt)
+
+        with st.chat_message("assistant"):
+            with st.spinner("ü§î Thinking..."):
+                try:
+                    st.write(response.response)
+                    message = {"role": "assistant", "content": response.response}
+                    st.session_state.messages.append({"role": "assistant", "content": response.response})
+                except Exception as e:
+                    st.error("I apologize, but I encountered an error. Please try rephrasing your question.")
